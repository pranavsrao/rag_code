# Databricks notebook source
# MAGIC %md
# MAGIC 1A. Creating the evaluation benchmark (request id, question, expected response) <br>
# MAGIC 1B. Creating the static answer sheet (request id, response)
# MAGIC ... you can run 1A and use it with `model_uri` argument
# MAGIC ... you can run 1A and 1B and use it `answer_sheet_table_name` 
# MAGIC
# MAGIC 2. Evaluate and report pre-POC numbers against post-POC 
# MAGIC ... let's simulate both pre-POC (dummy chain, no context) and post-POC numbers (our RAG chain with retrieved context)
# MAGIC ... (we do not care about human factual accuracy scores here) 
# MAGIC
# MAGIC 3. Demonstrate configuring `rag_eval.evaluate` with no-shot vs few-shot examples
# MAGIC ... let's only do this for a static answer sheet for post-POC factual accuracy scores
# MAGIC ... (we are showing how you can align to human factual accuracy scores here)

# COMMAND ----------

# MAGIC %md
# MAGIC ### Establish notebook parameters

# COMMAND ----------

dbutils.widgets.text("evaluation_benchmark_table_name", "", label="Evaluation Benchmark Table Name")

# COMMAND ----------

# MAGIC %md
# MAGIC ### Imports and configuration

# COMMAND ----------

import json
import os
import html
import yaml

import mlflow
import pyspark.sql.functions as F
from databricks import rag, rag_eval, rag_studio

# COMMAND ----------

mlflow.set_registry_uri('databricks-uc')

# COMMAND ----------

# MAGIC %sql
# MAGIC use catalog development;
# MAGIC use schema rag_studio;

# COMMAND ----------

# MAGIC %md 
# MAGIC ### Helper Functions

# COMMAND ----------

# MAGIC %run ../RAG_Experimental_Code

# COMMAND ----------

### START: Ignore this code, temporary workarounds given the Private Preview state of the product
from mlflow.utils import databricks_utils as du
os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR'] = "false"

def parse_deployment_info(deployment_info):
  browser_url = du.get_browser_hostname()
  message = f"""Deployment of {deployment_info.model_name} version {deployment_info.model_version} initiated.  This can take up to 15 minutes and the Review App & REST API will not work until this deployment finishes. 

  View status: https://{browser_url}/ml/endpoints/{deployment_info.endpoint_name}
  Review App: {deployment_info.rag_app_url}"""
  return message
### END: Ignore this code, temporary workarounds given the Private Preview state of the product

# COMMAND ----------

# MAGIC %md
# MAGIC ### Create a table for the evaluation benchmark
# MAGIC
# MAGIC The evaluation set represents the human-annotated ground truth data.
# MAGIC
# MAGIC | Column Name                  | Type                                              | Required? | Comment                                                                                                                                                  |
# MAGIC |------------------------------|---------------------------------------------------|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
# MAGIC | request_id                   | STRING                                            | Either `request_id` or `request` is required        | Id of the request (question)                                                                                                                             |
# MAGIC | request                     | STRING                                            |   Either `request_id` or `request` is required        | A request (question) to the RAG app, e.g., ‚ÄúWhat is Spark?‚Äù                                                                                              |
# MAGIC | expected_response            | STRING                                            |           | (Optional) The expected answer to this question                                                                                                          |
# MAGIC | expected_retrieval_context   | ARRAY<STRUCT<doc_uri: STRING, content: STRING>>   |           | (Optional) The expected retrieval context. The entries are ordered in descending rank. Each entry can record the URI of the retrieved doc and optionally the (sub)content that was retrieved. |
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC     
# MAGIC The responses to these questions can be provided either through an answer sheet (specified by the `answer_sheet_table_name` argument) or they can be generated by calling into an existing model (specified by `model_uri`). Exactly one of these two arguments to `rag_eval.evaluate` must be specified.
# MAGIC
# MAGIC If an answer sheet is used, then the schema of answer_sheet_table_name is expected to be as follows:
# MAGIC - `request_id`
# MAGIC - `app_version`
# MAGIC - `response`
# MAGIC - (optional) `retrieval_context`

# COMMAND ----------

evaluation_benchmark_table_name = dbutils.widgets.get("evaluation_benchmark_table_name")

evaluation_benchmark_fqdn = dbutils.notebook.run("./3_create_evaluation_benchmark", 300, arguments={"evaluation_benchmark_table_name":evaluation_benchmark_table_name})
# @note: Run 3_create_evaluation_benchmark and return the evaluation dataset table name.

# COMMAND ----------

# MAGIC %md
# MAGIC ### Evaluate dummy chain (simulating pre-POC) vs real RAG chain (simulating post-POC) using `model_URI` parameter:

# COMMAND ----------

############
# Currently, evaluation is slow with the Databricks provided LLM judge due to a limitation we are working to remove.  You can temporarily use any Model Serving endpoint to overcome this limitation, including DBRX.
############
config_json = {
    "assessment_judges": [
        {
            "judge_name": "databricks_eval_dbrx",
            "endpoint_name": "endpoints:/databricks-dbrx-instruct",
            "assessments": [
                "harmful",
                "faithful_to_context",
                "relevant_to_question_and_context",
                "relevant_to_question",
                "answer_good",
                "context_relevant_to_question",
            ],
        }
    ]
}

config_yml = yaml.dump(config_json)
print(config_yml)

# COMMAND ----------

# DBTITLE 1,Evaluate dummy chain (simulating pre-POC)
rag_chain_notebook_path = "3_rag_dummy_chain"
rag_chain_config_yaml   = "3_rag_chain_dummy_config.yaml"

dummy_model_uri = dbutils.notebook.run("./3_rag_chain_driver_small_notebook", 600,
                                 arguments={"rag_chain_notebook_path":rag_chain_notebook_path,
                                            "rag_chain_config_yaml":  rag_chain_config_yaml})

# COMMAND ----------

############
# Run evaluation, logging the results to a sub-run of the chain's MLflow run
############
with mlflow.start_run():
  evaluation_results_pre_poc = rag_eval.evaluate(eval_set_table_name = evaluation_benchmark_fqdn,
                                                 model_uri           = dummy_model_uri,
                                                 config              = config_yml)
# @todo: How to add app_version when using model_uri?

############
# Experimental: Log evaluation results to MLflow.  Note you can also use the dashboard produced by RAG Studio to view metrics/debug quality - it has more advanced functionality.
# Known issues: Can only be run once per run_id.
# ‚ö†Ô∏è‚ö†Ô∏è üêõüêõ Experimental features likely have bugs! üêõüêõ ‚ö†Ô∏è‚ö†Ô∏è
############
experimental_add_metrics_to_run(evaluation_results_pre_poc, evaluation_results_pre_poc.mlflow_run_id)
experimental_add_eval_outputs_to_run(evaluation_results_pre_poc, evaluation_results_pre_poc.mlflow_run_id)
experimental_add_eval_config_tags_to_run(evaluation_results_pre_poc, config_yml, evaluation_results_pre_poc.mlflow_run_id)

# Note: If you change the config after you log the model, but before you run this command, the incorrect config will be logged.
# RagConfig(chain_config_path).experimental_log_to_mlflow_run(run_id=evaluation_results.mlflow_run_id)

# COMMAND ----------

# DBTITLE 1,Evaluate real RAG chain (simulating post-POC)
rag_chain_notebook_path = "3_rag_chain"
rag_chain_config_yaml   = "3_rag_chain_config.yaml"

model_uri = dbutils.notebook.run("./3_rag_chain_driver_small_notebook", 600,
                                 arguments={"rag_chain_notebook_path":rag_chain_notebook_path,
                                  "rag_chain_config_yaml":  rag_chain_config_yaml})

# COMMAND ----------

############
# Run evaluation, logging the results to a sub-run of the chain's MLflow run
############
with mlflow.start_run():
  evaluation_results_post_poc = rag_eval.evaluate(eval_set_table_name = evaluation_benchmark_fqdn,
                                                  model_uri           = model_uri,
                                                  config              = config_yml)
# @todo: How to add app_version when using model_uri?

############
# Experimental: Log evaluation results to MLflow.  Note you can also use the dashboard produced by RAG Studio to view metrics/debug quality - it has more advanced functionality.
# Known issues: Can only be run once per run_id.
# ‚ö†Ô∏è‚ö†Ô∏è üêõüêõ Experimental features likely have bugs! üêõüêõ ‚ö†Ô∏è‚ö†Ô∏è
############
experimental_add_metrics_to_run(evaluation_results_post_poc, evaluation_results_post_poc.mlflow_run_id)
experimental_add_eval_outputs_to_run(evaluation_results_post_poc, evaluation_results_post_poc.mlflow_run_id)
experimental_add_eval_config_tags_to_run(evaluation_results_post_poc, config_yml, evaluation_results_post_poc.mlflow_run_id)

# Note: If you change the config after you log the model, but before you run this command, the incorrect config will be logged.
# RagConfig(chain_config_path).experimental_log_to_mlflow_run(run_id=evaluation_results.mlflow_run_id)

# COMMAND ----------

# DBTITLE 1,Report differences in metrics
pre_poc_model_uri   = dummy_model_uri
post_poc_model_uri  = model_uri

pre_poc_assessments = spark.table(evaluation_results_pre_poc.assessments_table_name).filter(F.col('app_version') == dummy_model_uri)
pre_poc_metrics     = spark.table(evaluation_results_pre_poc.eval_metrics_table_name).filter(F.col('app_version') == dummy_model_uri)
pre_poc_table       = pre_poc_metrics.join(pre_poc_assessments, ['request_id', 'app_version'])

post_poc_assessments = spark.table(evaluation_results_post_poc.assessments_table_name).filter(F.col('app_version') == model_uri)
post_poc_metrics     = spark.table(evaluation_results_post_poc.eval_metrics_table_name).filter(F.col('app_version') == model_uri)
post_poc_table       = post_poc_metrics.join(post_poc_assessments, ['request_id', 'app_version'])

# @note: This reads the assessments and metrics tables and filters down to just the required rows.

# COMMAND ----------

display(pre_poc_table.orderBy('request_id').limit(1))
display(post_poc_table.orderBy('request_id').limit(1))

# COMMAND ----------

pre_poc_table_unpack = pre_poc_table.\
                        withColumn('llm_factual_accuracy_score',  F.col('response_assessment').ratings.answer_good.double_value).\
                        withColumn('llm_factual_accuracy_reason', F.col('response_assessment').ratings.answer_good.rationale).\
                        select('request_id', 'request', 'expected_response', 'response', 
                               'llm_factual_accuracy_score', 'llm_factual_accuracy_reason',
                               F.lit('pre-POC').alias('state'))

display(pre_poc_table_unpack)
# @note: Unpack assessments in pre_poc table.

# COMMAND ----------

post_poc_table_unpack = post_poc_table.\
                        withColumn('llm_factual_accuracy_score',  F.col('response_assessment').ratings.answer_good.double_value).\
                        withColumn('llm_factual_accuracy_reason', F.col('response_assessment').ratings.answer_good.rationale).\
                        select('request_id', 'request', 'expected_response', 'response', 
                               'llm_factual_accuracy_score', 'llm_factual_accuracy_reason',
                               F.lit('post-POC').alias('state'))

display(post_poc_table_unpack)
# @note: Unpack assessments in pre_poc table.

# COMMAND ----------

factual_accuracy_table = pre_poc_table_unpack.union(post_poc_table_unpack.select(*pre_poc_table_unpack.columns))
display(factual_accuracy_table.groupBy('state').agg( (F.avg('llm_factual_accuracy_score')/5).alias('factual_accuracy_grade') ) )

# COMMAND ----------

# MAGIC %md
# MAGIC ### Evaluate dummy chain (simulating pre-POC) vs real RAG chain (simulating post-POC) using `answer_sheet_table_name` parameter:

# COMMAND ----------

# DBTITLE 1,Evaluate dummy chain (simulating pre-POC)
rag_chain_notebook_path = "3_rag_dummy_chain"
rag_chain_config_yaml   = "3_rag_chain_dummy_config.yaml"

answer_sheet_table_name = dbutils.notebook.run("./3_create_answer_sheet", 1200,
                                               arguments= {"evaluation_benchmark_fqdn":evaluation_benchmark_fqdn,
                                                          "rag_chain_notebook_path":rag_chain_notebook_path,
                                                          "rag_chain_config_yaml":rag_chain_config_yaml,
                                                          "app_version":"pre_poc_chain"})

# COMMAND ----------

############
# Run evaluation, logging the results to a sub-run of the chain's MLflow run
############
with mlflow.start_run():
  evaluation_results_pre_poc = rag_eval.evaluate(eval_set_table_name     = evaluation_benchmark_fqdn,
                                                 answer_sheet_table_name = answer_sheet_table_name,
                                                 config                  = config_yml)
# @todo: How to add app_version when using model_uri?

############
# Experimental: Log evaluation results to MLflow.  Note you can also use the dashboard produced by RAG Studio to view metrics/debug quality - it has more advanced functionality.
# Known issues: Can only be run once per run_id.
# ‚ö†Ô∏è‚ö†Ô∏è üêõüêõ Experimental features likely have bugs! üêõüêõ ‚ö†Ô∏è‚ö†Ô∏è
############
experimental_add_metrics_to_run(evaluation_results_pre_poc, evaluation_results_pre_poc.mlflow_run_id)
experimental_add_eval_outputs_to_run(evaluation_results_pre_poc, evaluation_results_pre_poc.mlflow_run_id)
experimental_add_eval_config_tags_to_run(evaluation_results_pre_poc, config_yml, evaluation_results_pre_poc.mlflow_run_id)

# Note: If you change the config after you log the model, but before you run this command, the incorrect config will be logged.
# RagConfig(chain_config_path).experimental_log_to_mlflow_run(run_id=evaluation_results.mlflow_run_id)

# COMMAND ----------

# DBTITLE 1,Evaluate real RAG chain (simulating post-POC)
rag_chain_notebook_path = "3_rag_chain"
rag_chain_config_yaml   = "3_rag_chain_config.yaml"

answer_sheet_table_name = dbutils.notebook.run("./3_create_answer_sheet", 1200,
                                               arguments={"evaluation_benchmark_fqdn":evaluation_benchmark_fqdn,
                                                          "rag_chain_notebook_path":rag_chain_notebook_path,
                                                          "rag_chain_config_yaml":rag_chain_config_yaml,
                                                          "app_version":"post_poc_chain"})

# COMMAND ----------

############
# Run evaluation, logging the results to a sub-run of the chain's MLflow run
############
with mlflow.start_run():
  evaluation_results_post_poc = rag_eval.evaluate(eval_set_table_name     = evaluation_benchmark_fqdn,
                                                  answer_sheet_table_name = answer_sheet_table_name,
                                                  config                  = config_yml)
# @todo: How to add app_version when using model_uri?

############
# Experimental: Log evaluation results to MLflow.  Note you can also use the dashboard produced by RAG Studio to view metrics/debug quality - it has more advanced functionality.
# Known issues: Can only be run once per run_id.
# ‚ö†Ô∏è‚ö†Ô∏è üêõüêõ Experimental features likely have bugs! üêõüêõ ‚ö†Ô∏è‚ö†Ô∏è
############
experimental_add_metrics_to_run(evaluation_results_post_poc, evaluation_results_post_poc.mlflow_run_id)
experimental_add_eval_outputs_to_run(evaluation_results_post_poc, evaluation_results_post_poc.mlflow_run_id)
experimental_add_eval_config_tags_to_run(evaluation_results_post_poc, config_yml, evaluation_results_post_poc.mlflow_run_id)

# Note: If you change the config after you log the model, but before you run this command, the incorrect config will be logged.
# RagConfig(chain_config_path).experimental_log_to_mlflow_run(run_id=evaluation_results.mlflow_run_id)

# COMMAND ----------

# DBTITLE 1,Report differences in metrics
pre_poc_app_version  = "pre_poc_chain"
post_poc_app_version = "post_poc_chain"

pre_poc_assessments = spark.table(evaluation_results_pre_poc.assessments_table_name).filter(F.col('app_version') == pre_poc_app_version)
pre_poc_metrics     = spark.table(evaluation_results_pre_poc.eval_metrics_table_name).filter(F.col('app_version') == pre_poc_app_version)
pre_poc_table       = pre_poc_metrics.join(pre_poc_assessments, ['request_id', 'app_version'])

post_poc_assessments = spark.table(evaluation_results_post_poc.assessments_table_name).filter(F.col('app_version') == post_poc_app_version)
post_poc_metrics     = spark.table(evaluation_results_post_poc.eval_metrics_table_name).filter(F.col('app_version') == post_poc_app_version)
post_poc_table       = post_poc_metrics.join(post_poc_assessments, ['request_id', 'app_version'])
# @note: This reads the assessments and metrics tables and filters down to just the required rows.

# COMMAND ----------

display(pre_poc_table.orderBy('request_id').limit(1))
display(post_poc_table.orderBy('request_id').limit(1))

# COMMAND ----------

pre_poc_table_unpack = pre_poc_table.\
                        withColumn('llm_factual_accuracy_score',  F.col('response_assessment').ratings.answer_good.double_value).\
                        withColumn('llm_factual_accuracy_reason', F.col('response_assessment').ratings.answer_good.rationale).\
                        select('request_id', 'request', 'expected_response', 'response', 
                               'llm_factual_accuracy_score', 'llm_factual_accuracy_reason',
                               F.lit('pre-POC').alias('state'))

display(pre_poc_table_unpack)
# @note: Unpack assessments in pre_poc table.

# COMMAND ----------

post_poc_table_unpack = post_poc_table.\
                        withColumn('llm_factual_accuracy_score',  F.col('response_assessment').ratings.answer_good.double_value).\
                        withColumn('llm_factual_accuracy_reason', F.col('response_assessment').ratings.answer_good.rationale).\
                        select('request_id', 'request', 'expected_response', 'response', 
                               'llm_factual_accuracy_score', 'llm_factual_accuracy_reason',
                               F.lit('post-POC').alias('state'))

display(post_poc_table_unpack)
# @note: Unpack assessments in pre_poc table.

# COMMAND ----------

factual_accuracy_table = pre_poc_table_unpack.union(post_poc_table_unpack.select(*pre_poc_table_unpack.columns))
display(factual_accuracy_table.groupBy('state').agg( (F.avg('llm_factual_accuracy_score')/5).alias('factual_accuracy_grade') ) )
