# Databricks notebook source
# MAGIC %md
# MAGIC DESCRIBE PURPOSE OF NOTEBOOK HERE

# COMMAND ----------

# MAGIC %md
# MAGIC ### Setup:

# COMMAND ----------

# MAGIC %md
# MAGIC #### Install dependencies:

# COMMAND ----------

# DBTITLE 1,Databricks RAG Studio Installer
# MAGIC %run ../wheel_installer

# COMMAND ----------

dbutils.library.restartPython() 

# COMMAND ----------

# MAGIC %md
# MAGIC #### Imports and configuration:

# COMMAND ----------

import os
import json
import html

import pandas as pd
import pyspark.sql.functions as F

import mlflow
from databricks import rag, rag_eval, rag_studio

# COMMAND ----------

mlflow.set_registry_uri('databricks-uc')

# COMMAND ----------

# MAGIC %md
# MAGIC #### Utility and experimental functions:

# COMMAND ----------

### START: Ignore this code, temporary workarounds given the Private Preview state of the product
from mlflow.utils import databricks_utils as du
os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR'] = "false"

def parse_deployment_info(deployment_info):
  browser_url = du.get_browser_hostname()
  message = f"""Deployment of {deployment_info.model_name} version {deployment_info.model_version} initiated.  This can take up to 15 minutes and the Review App & REST API will not work until this deployment finishes. 

  View status: https://{browser_url}/ml/endpoints/{deployment_info.endpoint_name}
  Review App: {deployment_info.rag_app_url}"""
  return message
### END: Ignore this code, temporary workarounds given the Private Preview state of the product

# COMMAND ----------

# MAGIC %run ../RAG_Experimental_Code

# COMMAND ----------

# MAGIC %md
# MAGIC ### Evaluate the chain

# COMMAND ----------

# MAGIC %md
# MAGIC #### First, build an evaluation set
# MAGIC
# MAGIC **NOTE**: Only run this section if changes are made to the evaluation benchmark.
# MAGIC
# MAGIC The evaluation set represents the human-annotated ground truth data.
# MAGIC
# MAGIC | Column Name                  | Type                                              | Required? | Comment                                                                                                                                                  |
# MAGIC |------------------------------|---------------------------------------------------|-----------|----------------------------------------------------------------------------------------------------------------------------------------------------------|
# MAGIC | request_id                   | STRING                                            | Either `request_id` or `request` is required        | Id of the request (question)                                                                                                                             |
# MAGIC | request                     | STRING                                            |   Either `request_id` or `request` is required        | A request (question) to the RAG app, e.g., “What is Spark?”                                                                                              |
# MAGIC | expected_response            | STRING                                            |           | (Optional) The expected answer to this question                                                                                                          |
# MAGIC | expected_retrieval_context   | ARRAY<STRUCT<doc_uri: STRING, content: STRING>>   |           | (Optional) The expected retrieval context. The entries are ordered in descending rank. Each entry can record the URI of the retrieved doc and optionally the (sub)content that was retrieved. |
# MAGIC
# MAGIC The responses to these questions can be provided either through an answer sheet (`answer_sheet_table_name`) or they can be generated by calling into an existing model (`model_uri`). Exactly one of these two parameters must be specified.
# MAGIC
# MAGIC If an answer sheet is used, then the schema of `answer_sheet_table_name` is expected to be as follows:
# MAGIC - `request_id`
# MAGIC - `app_version`
# MAGIC - (optional) `response`
# MAGIC - (optional) `retrieval_context`

# COMMAND ----------

# MAGIC %sql
# MAGIC use catalog development;
# MAGIC use database `rag_studio`;

# COMMAND ----------

benchmark = spark.table('grc_benchmark')
# @note: Contains the raw EdisonGPT benchmark data (request_id, request, expected_response, response, factual_accuracy).

display(benchmark)

# COMMAND ----------

eval_dataset = benchmark.select('request_id', 'request', 'expected_response')
# @note: Trimmed benchmark to only include request and expected response. 

display(eval_dataset)

# COMMAND ----------

responses = benchmark.\
                select('request_id', 'response').\
                withColumn('app_version',  F.lit('no_shot'))
# @note: Trimmed benchmark to only include the response.

display(responses)

# COMMAND ----------

eval_dataset.write.mode('overwrite').saveAsTable('eval_dataset_test')
responses.write.mode('overwrite').saveAsTable('responses_no_shot')
# @note: Write to Unity Catalog.

# COMMAND ----------

# MAGIC %md #### Run evaluation

# COMMAND ----------

############
# Turn the eval dataset into a Delta Table
############
# TODO: Change these values to your catalog and schema
uc_catalog          = "development"
uc_schema           = "rag_studio"
eval_table_name     = 'eval_dataset_test'
response_table_name = 'responses_no_shot'
eval_table_fqdn     = f"{uc_catalog}.{uc_schema}.{eval_table_name}"
response_table_fqdn = f"{uc_catalog}.{uc_schema}.{response_table_name}"

# COMMAND ----------

# DBTITLE 1,YAML Assessment Config Parser
import yaml

config_json = {
    "assessment_judges": [
        {
            "judge_name": "databricks_eval_dbrx",
            "endpoint_name": "endpoints:/databricks-dbrx-instruct",
            "assessments": [
                "relevant_to_question",
                "answer_good"
            ],
        }
    ]
}

config_yml = yaml.dump(config_json)
print(config_yml)

# COMMAND ----------

evaluation_results = rag_eval.evaluate(eval_set_table_name=eval_table_fqdn,
                                       answer_sheet_table_name=response_table_fqdn, 
                                       config=config_yml)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configure the evaluation
# MAGIC
# MAGIC Databricks provides a set of metrics that enable you to measure the quality, cost and latency of your RAG app. These metrics are curated by Databricks' Research team as the most relevant (no pun intended) metrics for evaluating RAG applications.
# MAGIC
# MAGIC RAG metrics can be computed using either:
# MAGIC 1. Human-labeled ground truth assessments
# MAGIC 2. LLM judge-labeled assessments 
# MAGIC
# MAGIC A subset of the metrics work only with *either* LLM judge-labeled OR human-labeled ground truth asessments.
# MAGIC
# MAGIC ### Improve judge accuracy
# MAGIC
# MAGIC To improve the accuracy of the Databricks judges, you can provide few-shot examples of "good" and "bad" answers for each LLM judge.  Databricks strongly reccomends providing at least 2 postive and 2 negative examples per judge to improve the accuracy.  See the bottom of the notebook [`5_evaluation_without_rag_studio`](M1_Sample_Code/5_evaluation_without_rag_studio.py) for how to do this.  *Note: Even though this example configuration is included in the non-RAG Studio evaluation example, you can use the example configuration with this notebook.*
# MAGIC
# MAGIC
# MAGIC ### Unstructured docs retrieval & generation metrics
# MAGIC
# MAGIC #### Retriever
# MAGIC
# MAGIC RAG Studio supports the following metrics for evaluating the retriever.
# MAGIC
# MAGIC | Question to answer                                                                | Metric | Per trace value | Aggregated value | Work with human assessments | LLM judged assessments & judge name | 
# MAGIC |-----------------------------------------------------------------------------------|--------|--------|--------|------|--------|
# MAGIC | Are the retrieved chunks relevant to the user’s query?                            | Precision of "relevant chunk" @ K | 0 to 100% | 0 to 100% | ✔️ | ✔️ `context_relevant_to_question` |
# MAGIC | Are **ALL** chunks that are relevant to the user’s query retrieved?               | Recall of "relevant chunk" @ K | 0 to 100% |0 to 100% | ✔️ |✖️ |
# MAGIC | Are the retrieved chunks returned in the correct order of most to least relevant? | nDCG of "relevant chunk" @ K | 0 to 1 | 0 to 1 |✔️ | ✖️ |
# MAGIC
# MAGIC #### Generation model
# MAGIC
# MAGIC These metrics measure the generation model's performance when the prompt is augemented with unstructured docs from a retrieval step.
# MAGIC
# MAGIC | Question to answer                                                                | Metric | Per trace value | Aggregated value | Work with human assessments | LLM judged assessments & judge name | 
# MAGIC |-----------------------------------------------------------------------------------|--------|--------|--------|------|--------|
# MAGIC | Is the LLM not hallucinating & responding based ONLY on the context provided? | Faithfulness (to context) | true/false | 0 to 100% | ✖️ | ✔️ `faithful_to_context` |
# MAGIC | Is the response on-topic given the query AND retrieved contexts? | Answer relevance (to query given the context) | true/false | 0 to 100% | ✖️ | ✔️ `relevant_to_question_and_context` | 
# MAGIC | Is the response on-topic given the query? | Answer relevance (to query) | true/false | 0 to 100% | ✖️ | ✔️ `relevant_to_question` | 
# MAGIC | What is the cost of the generation? | Token Count | sum(tokens) | sum(tokens) | n/a |n/a |
# MAGIC | What is the latency of generation? | Latency | milliseconds | average(milliseconds) | n/a | n/a |
# MAGIC
# MAGIC #### RAG chain metrics
# MAGIC
# MAGIC These metrics measure the chain's final response back to the user.  
# MAGIC
# MAGIC | Question to answer                                                                | Metric | Per trace value | Aggregated value | Work with human assessments | LLM judged assessments & judge name | 
# MAGIC |-----------------------------------------------------------------------------------|--------|--------|--------|------|--------|
# MAGIC | Is the response accurate (correct)? | Answer correctness (vs. ground truth) | true/false | 0 to 100% |✔️ `answer_good` | ✖️ |
# MAGIC | Does the response violate any of my company policies (racism, toxicity, etc)? | Toxicity | true/false | 0 to 100% | ✖️ | ✔️ `harmful` |
# MAGIC
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC #### Create a separate table with responses for few-shot examples-configured LLM judge-labeled assessment:

# COMMAND ----------

responses_few_shot = spark.table('grc_benchmark').\
                      select('request_id', 'response').\
                      withColumn('app_version', F.lit('few_shot'))

display(responses_few_shot)

# COMMAND ----------

responses_few_shot.write.mode('overwrite').saveAsTable('responses_few_shot')

# COMMAND ----------

# MAGIC %md
# MAGIC #### Configure LLM judge-based assessment with few-shot examples:

# COMMAND ----------

############
# Turn the eval dataset into a Delta Table
############
# TODO: Change these values to your catalog and schema
uc_catalog          = "development"
uc_schema           = "rag_studio"
eval_table_name     = 'eval_dataset_test'
response_table_name = 'responses_few_shot'
eval_table_fqdn     = f"{uc_catalog}.{uc_schema}.{eval_table_name}"
response_table_fqdn = f"{uc_catalog}.{uc_schema}.{response_table_name}"

# COMMAND ----------

config_json_2 = {
    "assessment_judges": [
        {
            # "judge_name": "databricks_eval",
            "judge_name": "databricks_eval_dbrx",
            "endpoint_name": "endpoints:/databricks-dbrx-instruct",
            "assessments": [
                "relevant_to_question",
                {
                    "answer_good": {
                        "examples": [
                            {
                                "request": "What are SCE's wildfire safety measures?",
                                "response": """The document outlines Southern California Edison Company’s (SCE) wildfire safety measures as follows: System Hardening: SCE’s Grid Safety & Resiliency Program (GSRP) and Wildfire Mitigation Program (WMP) include grid resiliency measures, primarily the Wildfire Covered Conductor Program (WCCP), which involves replacing and insulating formerly bare utility wires to reduce ignition risks12. Situational Awareness: SCE is enhancing situational awareness through high-definition cameras and weather stations to detect and monitor potential ignitions, aiming to prevent fire spread3. Inspections and Vegetation Management: SCE has expanded infrastructure inspection programs, such as the Enhanced Overhead Inspection (EOI) program, and increased vegetation management to improve pruning clearances and remove hazardous trees4. Public Outreach and Operational Practices: SCE educates customers about wildfire threats and coordinates with local fire agencies and communities5. During high-risk periods, SCE may proactively de-energize lines through the Public Safety Power Shutoff (PSPS) program6.
                                """,
                                "expected_response": """The document outlines Southern California Edison Company’s (SCE) measures to mitigate wildfires and the associated costs as follows. System Hardening: SCE’s Grid Safety & Resiliency Program (GSRP) and Wildfire Mitigation Program (WMP) include grid resiliency measures like the Wildfire Covered Conductor Program (WCCP) to replace and insulate wires. Situational Awareness: Implementation of high-definition cameras and weather stations to detect and monitor ignitions. Vegetation Management: Expanded programs to increase pruning clearances and remove hazardous trees. Public Outreach and PSPS: Educating customers about wildfire threats and selectively de-energizing lines through the Public Safety Power Shutoff (PSPS) program.""",
                                "value": True,
                                "rationale": "The output details all of the main points covered in the ground truth/expected response",
                            },
                            {
                                "request": "What does SCE stand for?",
                                "response": "South Carolina Electric Company",
                                "expected_response": "Southern California Edison Company",
                                "value": False,
                                "rationale": "The output does not match with the ground truth / expected response.",
                            },
                        ]
                    }
                }
            ],
        }
    ]
}

config_custom_judge_yml = yaml.dump(config_json_2)

# COMMAND ----------

custom_judge_evaluation_results = rag_eval.evaluate(eval_set_table_name=eval_table_fqdn,
                                                    answer_sheet_table_name=response_table_fqdn, 
                                                    config=config_custom_judge_yml)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Determine difference in using no-shot vs few-shot examples for configuring LLM-as-a-judge:
# MAGIC
# MAGIC This section is intended to measure whether LLM-judged scores align better to human-judged scores after chaging the evaluation configuration.

# COMMAND ----------

eval_dataset_assessments = spark.table('development.rag_studio.eval_dataset_test_assessments')

display(eval_dataset_assessments)

# COMMAND ----------

factual_accuracy_scores = spark.table('grc_benchmark').\
                      select('request_id', 'factual_accuracy', 'POC')

display(factual_accuracy_scores)

# COMMAND ----------

factual_accuracy_scores_pdf = factual_accuracy_scores.toPandas()

# Change all 0 scores to 1 to align Human rubric to LLM rubric
factual_accuracy_scores_pdf['factual_accuracy'] = [1 if item == 0 else item for item in factual_accuracy_scores_pdf['factual_accuracy']]

# Rename column from factual_accuracy to human_score
factual_accuracy_scores_pdf = factual_accuracy_scores_pdf.rename(columns={'factual_accuracy': 'human_score'})

# COMMAND ----------

eval_pdf = eval_dataset_assessments.toPandas()

eval_pdf = eval_pdf.drop(columns=['source_type', 'source_id', 'source_metadata', 'retrieval_assessment'])

# COMMAND ----------

df1 = eval_pdf[eval_pdf['app_version'] == 'no_shot']
df2 = eval_pdf[eval_pdf['app_version'] == 'few_shot']
dfs = [df1, df2]

for i in dfs:
    answer_good = []
    metrics = i['response_assessment']
    for metric in metrics:
        answer_good.append(metric['ratings']['answer_good']['double_value'])
    i['answer_good'] = answer_good

df1 = df1.drop(columns=['app_version', 'response_assessment'])
df1 = df1.rename(columns = {'answer_good': 'no_shot_answer_good'})
df1 = df1.sort_values('request_id')
df1 = df1.reset_index(drop=True)
#df1['human_score'] = fixed_factual_accuracy_scores

df2 = df2.drop(columns=['app_version', 'response_assessment'])
df2 = df2.rename(columns = {'answer_good': 'few_shot_answer_good'})
df2 = df2.sort_values('request_id')
df2 = df2.reset_index(drop=True)

# COMMAND ----------

results = pd.merge(df1, df2, on='request_id',  how='left')
results['request_id'] = results['request_id'].astype('float64')

# COMMAND ----------

final_df = pd.merge(results, factual_accuracy_scores_pdf, on = 'request_id', how='left')

# COMMAND ----------

no_shot_squared_error = (final_df['no_shot_answer_good']-final_df['human_score'])**2
few_shot_squared_error = (final_df['few_shot_answer_good']-final_df['human_score'])**2

final_df['no_shot_squared_error'] = no_shot_squared_error
final_df['few_shot_squared_error'] = few_shot_squared_error

final_df

# COMMAND ----------

summary = pd.DataFrame(
    {
        'no_shot_rmse': (final_df['no_shot_squared_error'].mean())**0.5,
        'few_shot_rmse': (final_df['few_shot_squared_error'].mean())**0.5
    },
    index =[0]
)

summary

# COMMAND ----------

# MAGIC %md
# MAGIC ### Compare factual accuracy scores of pre- and post-POC responses using RAG Studio LLM-as-a-judge

# COMMAND ----------

POC_assignments = spark.table('grc_benchmark').\
                      select('request_id', 'POC')

display(POC_assignments)

# COMMAND ----------

poc_pdf = POC_assignments.toPandas()

# COMMAND ----------

df2['request_id'] = df2['request_id'].astype('float64')
llm_scores = pd.merge(df2, poc_pdf, on = 'request_id', how='left')
llm_scores = llm_scores.rename(columns={'few_shot_answer_good': 'answer_good'})

# COMMAND ----------

llm_scores

# COMMAND ----------

pre_df = llm_scores[llm_scores['POC'] == 'pre']
post_df = llm_scores[llm_scores['POC'] == 'post']

pre_len = len(pre_df)
post_len = len(post_df)

pre_answer_good_score = pre_df['answer_good'].sum()/(pre_len*5)
post_answer_good_score = post_df['answer_good'].sum()/(post_len*5)

summary = pd.DataFrame(
    {
        'Pre-POC': pre_answer_good_score,
        'Post-POC': post_answer_good_score
    },
    index =[0]
)

summary
